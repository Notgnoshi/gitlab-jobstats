#!/usr/bin/env python3
"""Calculate and plot statistics for job data generated by jobstats.py."""

import argparse
import collections
import csv
import datetime
import fnmatch
import itertools
import logging
import operator
import statistics
import sys
from typing import Dict, Iterable, List, Tuple


def parse_args():
    parser = argparse.ArgumentParser(
        description=__doc__, formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument(
        "--log-level",
        "-l",
        type=str,
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
        help="Set the logging output level. Defaults to INFO.",
    )
    parser.add_argument(
        "input",
        type=argparse.FileType("r"),
        help="Input CSV from jobstats.py",
    )
    parser.add_argument(
        "--jobs",
        "-j",
        nargs="*",
        default="*",
        help="Job name globs to include. May be given multiple times",
    )
    # TODO: Should there be an --exclude glob?
    parser.add_argument(
        "--plot-failures",
        "-f",
        action="store_true",
        help="Generate a timeseries plot. Requires matplotlib+seaborn",
    )
    parser.add_argument(
        "--plot-durations",
        "-d",
        action="store_true",
        help="Generate a job duration plot. Requires matplotlib+seaborn",
    )

    return parser.parse_args()


def main(args):
    jobs = csv.DictReader(args.input)
    jobs = [j for j in jobs if any(fnmatch.fnmatchcase(j["name"], pat) for pat in args.jobs)]
    for job in jobs:
        job["created-date"] = datetime.datetime.fromisoformat(job["created-date"])

    summarize(jobs)
    most_common_failures(jobs)

    if args.plot_failures:
        plot_failures(jobs)

    if args.plot_durations:
        plot_durations(jobs)


def count_by_status(jobs: List[Dict], status: str) -> int:
    return sum(1 for j in jobs if j["status"] == status)


def summarize(jobs: List[Dict]):
    """Calculate a basic summary."""
    total = len(jobs)
    num_success = count_by_status(jobs, "success")
    percent_success = (num_success / total) * 100
    num_failed = count_by_status(jobs, "failed")
    percent_failed = (num_failed / total) * 100

    print(f"total jobs:  {total}")
    print(f"    success:  {num_success} ({percent_success:.2f}%)")
    print(f"    failure:  {num_failed} ({percent_failed:.2f}%)")

    jobs = (j for j in jobs if j["status"] == "success")
    durations = [float(j["duration"]) for j in jobs]
    mean = statistics.mean(durations)
    median = statistics.median(durations)
    stdev = statistics.stdev(durations)

    print("successful job durations:")
    print(f"    count:  {len(durations)}")
    print(f"    min:    {min(durations):=.2f}s")
    print(f"    max:    {max(durations):=.2f}s")
    print(f"    mean:   {mean:=.2f}s")
    print(f"    median: {median:=.2f}s")
    print(f"    stdev:  {stdev:=.2f}s")


def most_common_failures(jobs: List[Dict]):
    """Determine the job(s) that fail the most frequently."""
    failed_jobs = (j for j in jobs if j["status"] == "failed")
    failed_job_names = (j["name"] for j in failed_jobs)
    counter = collections.Counter(failed_job_names)

    most_common = counter.most_common()
    most_common.sort(key=operator.itemgetter(1), reverse=True)
    if len(most_common) < 2:
        return

    print("job breakdown:")
    for job, num_failures in most_common:
        print(f"    failures: {num_failures} job: {job}")


def month_iter(start: datetime.datetime, stop: datetime.datetime) -> Iterable[datetime.date]:
    """Iterate over the months in between the given inclusive datetimes."""
    ym_start = 12 * start.year + start.month - 1
    ym_end = 12 * stop.year + stop.month - 1
    for ym in range(ym_start, ym_end):
        y, m = divmod(ym, 12)
        yield datetime.date(year=y, month=m + 1, day=1)


def day_iter(start: datetime.datetime, stop: datetime.datetime) -> Iterable[datetime.date]:
    """Iterate over the days in between the given inclusive datetimes."""
    current = start.date()
    end = stop.date()
    while current <= end:
        yield current
        current += datetime.timedelta(days=1)


def fill_under_lines(ax, alpha=0.2, **kwargs):
    for line in ax.lines:
        x, y = line.get_xydata().T
        ax.fill_between(x, 0, y, color=line.get_color(), alpha=alpha, **kwargs)


def plot_failures(jobs: List[Dict]):
    """Plot a stacked timeseries of successful and failed jobs over time."""
    import matplotlib.pyplot as plt
    import seaborn as sns

    logging.getLogger("matplotlib").setLevel(logging.WARNING)
    sns.set_theme()

    # Time is continuous, so build out an array of all time buckets between start and stop dates
    dates = sorted(j["created-date"] for j in jobs)
    date_range_days = (dates[-1] - dates[0]).days
    use_daily = date_range_days < 60  # Use daily buckets if less than ~2 months

    if use_daily:
        buckets = [date.strftime("%Y-%m-%d") for date in day_iter(dates[0], dates[-1])]
        bucket_fmt = "%Y-%m-%d"
        x_label = "Day"
    else:
        buckets = [f"{date.year}-{date.month}" for date in month_iter(dates[0], dates[-1])]
        bucket_fmt = None  # Custom format for months
        x_label = "Month"

    # Aggregate jobs by time bucket
    jobs_by_bucket: Dict[str, List[Dict]] = collections.defaultdict(list)
    for job in jobs:
        dt = job["created-date"]
        if use_daily:
            key = dt.strftime(bucket_fmt)
        else:
            key = f"{dt.year}-{dt.month}"
        jobs_by_bucket[key].append(job)

    total_by_bucket = [len(jobs_by_bucket[b]) for b in buckets]
    success_by_bucket = [count_by_status(jobs_by_bucket[b], "success") for b in buckets]
    failed_by_bucket = [count_by_status(jobs_by_bucket[b], "failed") for b in buckets]

    sns.lineplot(x=buckets, y=total_by_bucket, label="total")
    sns.lineplot(x=buckets, y=failed_by_bucket, label="failed")
    sns.lineplot(x=buckets, y=success_by_bucket, label="success")

    fill_under_lines(plt.gca())

    plt.title("CI/CD Jobs Over Time")
    plt.xlabel(x_label)
    plt.ylabel("Count")
    plt.legend(loc="upper right")
    plt.xticks(rotation=45 if use_daily else 0)
    plt.tight_layout()
    plt.show()


def sliding_window(iterable: Iterable, size: int) -> Iterable[Tuple]:
    """Generate an iterable of sliding windows over the given iterable."""
    iterator = iter(iterable)
    window = collections.deque(itertools.islice(iterator, size - 1), maxlen=size)
    for item in iterator:
        window.append(item)
        yield tuple(window)


def rolling_average(data: List[float], window_size: int = 10) -> List[float]:
    """Calculate the backwards-looking rolling average of the given data."""
    window_size = min(len(data), window_size)
    result = []

    # Prime the sliding window pump
    for i in range(1, window_size):
        result.append(statistics.mean(data[:i]))

    # Now do the rest with the sliding window
    for window in sliding_window(data, window_size):
        result.append(statistics.mean(window))
    assert len(result) == len(data)
    return result


def plot_durations(jobs: List[Dict]):
    """Plot successful job durations."""
    import matplotlib.pyplot as plt
    import seaborn as sns

    logging.getLogger("matplotlib").setLevel(logging.WARNING)
    sns.set_theme()

    jobs = sorted((j for j in jobs if j["status"] == "success"), key=lambda j: j["created-date"])
    durations = [float(j["duration"]) for j in jobs]
    queued = [float(j["queued-duration"]) for j in jobs]

    # Convert to minutes if max duration is over 5 minutes
    max_duration = max(durations)
    use_minutes = max_duration > 5 * 60
    if use_minutes:
        durations = [d / 60 for d in durations]
        queued = [q / 60 for q in queued]
        y_label = "minutes"
    else:
        y_label = "seconds"

    durations_avg = rolling_average(durations, window_size=20)
    queued_avg = rolling_average(queued, window_size=20)
    x = range(0, len(durations))

    _fig, ax = plt.subplots()
    sns.scatterplot(ax=ax, x=x, y=durations, color="C0", alpha=0.6)
    sns.lineplot(ax=ax, x=x, y=durations_avg, label="duration", color="C0")

    sns.scatterplot(ax=ax, x=x, y=queued, color="C1", alpha=0.6)
    sns.lineplot(ax=ax, x=x, y=queued_avg, label="queued", color="C1")

    # Set x-axis to show only first and last dates
    first_date = jobs[0]["created-date"].strftime("%Y-%m-%d")
    last_date = jobs[-1]["created-date"].strftime("%Y-%m-%d")
    ax.set_xticks([0, len(durations) - 1])
    ax.set_xticklabels([first_date, last_date])

    plt.title("CI/CD Job Durations Over Time")
    plt.xlabel("time")
    plt.ylabel(y_label)
    plt.legend(loc="upper right")
    plt.show()


if __name__ == "__main__":
    args = parse_args()
    fmt = "%(asctime)s %(module)s %(levelname)s: %(message)s"
    logging.basicConfig(
        format=fmt,
        datefmt="%Y-%m-%dT%H:%M:%S%z",
        level=args.log_level,
        stream=sys.stderr,
    )
    # Color log output if possible, because I'm a sucker
    try:
        import coloredlogs

        coloredlogs.install(fmt=fmt, level=args.log_level, datefmt="%Y-%m-%dT%H:%M:%S%z")
    except ImportError:
        pass
    main(args)
